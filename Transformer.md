The following are the neural network building blocks (functions with learnable parameters) from which transformers are made. Full architectures featuring these building blocks are presented in the next section. (By a slight abuse of notation, we identify ğ‘‰ with the set {1, 2, . . . , ğ‘V }.)

Token embedding. The token embedding learns to represent each vocabulary element as a vector in â„^ğ‘‘_e; see Algorithm 1.

| Algorithm 1: Token embedding. |
| ----------------------------- |
| Input: ğ‘£ âˆˆ ğ‘‰ â‰… [ğ‘_ğ‘‰], a token ID. |
| Output: ğ’† âˆˆ â„^ğ‘‘_e, the vector representation of the Token. |
| Parameters: ğ‘¾_ğ’† âˆˆ â„^(ğ‘‘_e Ã— ğ‘_ğ‘‰), the token embedding matrix. |
| 1 return ğ’† = ğ‘¾_ğ’†[:,ğ‘£] |

| Symbol | Type | Explanation |
| ------ | ---- | ----------- |
| ğ‘‰ | â‰… [ ğ‘_V ] | vocabulary |
| ğ‘¾_ğ’† | âˆˆ â„^(ğ‘‘_e Ã— ğ‘_V) | token embedding matrix |

 Algorithm 2: Positional embedding.
  Input:  âˆˆ [ max ], position of a token in
         the sequence.
  Output: ğ’† ğ’‘ âˆˆ â„ğ‘‘e , the vector
            representation of the position.
  Parameters: ğ‘¾ğ’‘ âˆˆ â„ğ‘‘e Ã—max , the positional
                embedding matrix.
1 return ğ’† ğ’‘ = ğ‘¾ ğ’‘ [:, ]